{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 8: Fitting Data\n",
    "In this lab we will look at some of the basic ideas and issues associated with fitting data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bring in our imports\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "from scipy.optimize import curve_fit\n",
    "from sklearn import linear_model\n",
    "from matplotlib import pyplot as plt\n",
    "#%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting Lines and Polynomials\n",
    "Fitting linear functions can be done in an exact way using a number of linear algebra techniques. This suite of algorithms is called linear least-squares fitting. Most of these techniques assume that the error in y is signficant and that the error in x is not. Numpy has a very useful method for this called polyfit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## An Example of a Line\n",
    "This is a line $y=mx+b$ where $m = 1.3$ $b=-2.7$ with variable error bars added on. \n",
    "Let's Create our Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create Some data\n",
    "np.random.seed(100) #Seed the random number generator, so the result is always the same\n",
    "num = 20\n",
    "x=np.linspace(-5,5,num) #Create an evenly spaced set of data points\n",
    "m= 1.3\n",
    "b = -2.7\n",
    "theory_y = m*x + b #Let's make a line\n",
    "\n",
    "yerr = np.random.rand(num) + 1 # a series of sigmas (uncertainties) between 1 and 2\n",
    "y = yerr * np.random.standard_normal(num) + theory_y #Move my y values by the error\n",
    "\n",
    "# Plot the data\n",
    "ls = 18 #label fontsize\n",
    "ts = 15 #tick label fontsize\n",
    "f = plt.figure()                            # define figure\n",
    "f.suptitle(\"y = mx + b\",fontsize=ls*1.05)   # set figure title\n",
    "ax = f.add_subplot(111)                             # add a subplot (just one) to the figure\n",
    "ax.errorbar(x, y, yerr=yerr,fmt='o',label=\"Data\")   # plot data & errors\n",
    "ax.plot(x,theory_y,label=\"Theory\")                  # plot theoretical line\n",
    "#ax.set_aspect(\"equal\")                              # set aspect ratio\n",
    "ax.set_xlabel(\"x\",fontsize=ls,labelpad=10)          # set axes labels\n",
    "ax.set_ylabel(\"y\",fontsize=ls,rotation=0,labelpad=10)\n",
    "ax.set_xticks(np.arange(-4,6,2))                    # set axes ticks\n",
    "ax.set_yticks([-10,-5,0,5])\n",
    "ax.tick_params(axis=\"both\",which=\"major\",labelsize=ts,direction=\"out\",pad=5,right=False,top=False) # adjust ticks\n",
    "ax.tick_params(axis=\"x\",rotation=0,color=\"k\",labelcolor=\"k\",width=1,length=4)\n",
    "ax.legend()                                         # add the legend\n",
    "plt.show()                                          # display the entire figure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Polyfit, scikit-learn\n",
    "Polyfit is very good for fitting lines or polynomials. Inputs are np.polyfit(x,y,degree,w=weights). Where weight is equal to 1/sigma. Returns the coefficients of the fit highest power first. You can use np.polyval() to plot the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"From our theoretical equation, we defined: m = {:.1f}, b = {:.1f}\".format(m,b))\n",
    "\n",
    "# Fit with numpy polyfit\n",
    "(param,cov) = np.polyfit(x,y,1,w=1/yerr,cov=True)\n",
    "print(\"with numpy.polyfit() we predict: m = {:.3f}, b = {:.3f}\".format(param[0],param[1]))\n",
    "# Predict line values with polyfit params:\n",
    "fitp = np.poly1d(param)\n",
    "ypolyfit = fitp(x)\n",
    "\n",
    "# Fit with scikit=learn LinearRegressor\n",
    "X = np.reshape(x,(len(x),1))\n",
    "lr = linear_model.LinearRegression()\n",
    "lr.fit(X,y,1/yerr)\n",
    "print(\"with sklearn.linear_model.LinearRegression().fit() we predict: m = {:.3f}, b = {:.3f}\".format(lr.coef_[0],lr.intercept_))\n",
    "# Predict line values with sklearn params:\n",
    "ylrfit = regr.predict(X)\n",
    "\n",
    "\n",
    "\n",
    "# Plot the results\n",
    "ls = 18\n",
    "ts = 15\n",
    "f = plt.figure(figsize=(7,7))\n",
    "f.suptitle(\"y = mx + b\",fontsize=ls*1.05)\n",
    "ax = f.add_subplot(111)\n",
    "ax.errorbar(x, y, yerr=yerr,fmt='o',label='Data (\"Measured\")')\n",
    "ax.plot(x,theory_y,label=\"Theory\")\n",
    "ax.plot(x, ypolyfit,'r--',lw=1,label=\"Model (using numpy polyfit)\")\n",
    "ax.plot(x,ylrfit,lw=4,alpha=0.5,label=\"Model (using sklearn LinearRegressor)\")\n",
    "ax.set_xlabel(\"x\",fontsize=ls,labelpad=10)\n",
    "ax.set_ylabel(\"y\",fontsize=ls,rotation=0,labelpad=10)\n",
    "ax.set_xticks(np.arange(-4,6,2))\n",
    "ax.set_yticks([-10,-5,0,5])\n",
    "ax.tick_params(axis=\"both\",which=\"major\",labelsize=ts,direction=\"in\",pad=5,right=True,top=True)\n",
    "ax.tick_params(axis=\"x\",rotation=0,color=\"k\",labelcolor=\"k\",width=1,length=4)\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Covariance and Correlation Matrices\n",
    "Now we would like an estimate of how good our parameter estimation is. In order to do this we need what is called the error or covariance matrix. The covariance matrix is a square, symmetric matrix that is constructed from the variances in each parameter. \n",
    "\n",
    "For a 2 parameter ($p_1,p_2$) fit: $cov = \\begin{bmatrix} \\sigma_{p_1}^2 & \\sigma_{p_1p_2}^2 \\\\ \\sigma_{p_2p_1}^2 & \\sigma_{p_2}^2 \\\\ \\end{bmatrix}$ Where $\\sigma_{p_jp_k}^2 =$ covariance$(p_j,p_k)$ The diagonal gives us the variance in each parameter.\n",
    "\n",
    "The correlation matrix tells you how correlated each variable is: $cor = \\begin{bmatrix} \\frac{\\sigma_{p_1}^2}{\\sigma_{p_1}\\sigma_{p_1}} & \\frac{\\sigma_{p_1p_2}^2}{\\sigma_{p_1}\\sigma_{p_1}} \\\\ \\frac{\\sigma_{p_1p_2}^2}{\\sigma_{p_1}\\sigma_{p_2}}& \\frac{\\sigma_{p_2}^2}{\\sigma_{p_2}\\sigma_{p_2}} \\\\ \\end{bmatrix}$ \n",
    "\n",
    "It is normalized to run from -1 to 1, with -1 being completely anti-correlated, 0 not correlated, and 1 being completely correlated.\n",
    "\n",
    "*From Katie's Notes:*\n",
    "\n",
    "*Covariance describes how two variables change in relation to each other, as variance is a measure of how a single variable varies (the variability, or a measure of the spread of data around the mean).*\n",
    "\n",
    "*Correlation is not only the direction of the relationship, but the strength.*\n",
    "\n",
    "*A covariance matrix is a square, symmetric matrix that is constructed from the variances in each parameter.  The diagonal gives the variances, and the other elements are covariances between the parameters.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cov2cor(cov):\n",
    "    n = len(cov[0])\n",
    "    diag = np.sqrt(np.diag(cov))\n",
    "    a = np.vstack([diag]*n)\n",
    "    b = diag.reshape(n,1)\n",
    "    c = np.hstack([b]*n) * a\n",
    "    inv = 1/c\n",
    "    return(inv * cov)\n",
    "\n",
    "print(\"Covariance Matrix: \")\n",
    "print(cov)\n",
    "cor = cov2cor(cov)\n",
    "print(\"Correlation Matrix: \")\n",
    "print(cor)\n",
    "print(\"m= {:.3f} +/- {:.3f}\\nb= {:.3f} +/- {:.3f}\".format(param[0],np.sqrt(cov[0,0]),param[1],np.sqrt(cov[1,1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that our variables are not very correlated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How Good is Our Fit?\n",
    "Our line goes through the points, but can we quantify how good our fit is? If we know our what our answer is, we can use Percent error $\\frac{|Theory-Observed|}{Theory} \\times 100\\%$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Percent Error: m: {:.1%} b: {:.1%}\".format(np.abs((param[0]-m)/m), np.abs((param[1]-b)/b)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general, however, we have to use the data we have in order to evaluate the quality of the fit. The first thing we can do is look at the residual of our fit by subtracting our observed y values from the fitted y values. Then we take the standard deviation. This is called the Root Mean Squared error or RMS error. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rms = np.sqrt(np.mean(((y-ypolyfit)-np.mean(y-ypolyfit))**2))\n",
    "rms_numpy = np.std(y-ypolyfit)\n",
    "print(\"RMS =  {:.10f}, RMS(numpy) = {:.10f}\".format(rms,rms_numpy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls = 14\n",
    "ts = 12\n",
    "plt.figure(figsize=(7,7))\n",
    "plt.title(\"Residuals\",fontsize=ls*1.1)\n",
    "plt.axvline(0,c=\"k\",alpha=0.4,ls=\"dashed\") # plot vertical/horizontal lines at 0,0\n",
    "plt.axhline(0,c=\"k\",alpha=0.4,ls=\"dashed\")\n",
    "plt.plot(ypolyfit,ypolyfit,c=\"k\",alpha=0.4,ls=\"dashed\",marker=\".\") # plot a line for theoretical y values\n",
    "for i,j in zip(ypolyfit,y): plt.plot([i,i],[i,j],c=\"k\",alpha=0.4,lw=1) # plot horizontal line between each theoretical,predicted y pair\n",
    "plt.scatter(ypolyfit,y) # plot theoretical vs predicted y values\n",
    "plt.xlabel(\"y, fit (from y = mx + b)\",fontsize=ls)\n",
    "plt.ylabel(\"y, data\",fontsize=ls)\n",
    "plt.tick_params(axis=\"both\",labelsize=ts)\n",
    "plt.gca().set_aspect(\"equal\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use this RMS as a systemic error. If we use our fitted relation to make preditictions for y values based on x values, we know that $\\sigma_{sys}$ is always there of 1.275."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The RMS method that we used can't tell us how good our fit is compared to how good it should be based on the y-errors. We fundamentallly want to know whether this is a \"good\" fit or not.\n",
    "\n",
    "The standard method used in astronomy to determine a good fit is the $\\chi^2$ metric. $\\chi^2 = \\sum_{i=1}^{N} \\left[ \\frac{y_{obs,i} - y_{fit,i}}{\\sigma_i}\\right]^2$\n",
    "\n",
    "*From Katie's Notes:*\n",
    "\n",
    "*RMS error is the square root of the variance, or a measure of how much our model residuals vary from the model.  Gives us systematic error, but doesnâ€™t take into account the actual uncertainties in each measurement.  To get that, we use the chi square value, and that in turn is affected by the # of data points (n) and the # of parameters (p).*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Chi Squared value\n",
    "chi = np.sum(((y - ypolyfit)/yerr)**2)\n",
    "print(\"Chisq of data ({:d} data points): {:.3f}\".format(num,chi))\n",
    "# Create an ideal distribution of residuals (y - yfit)/yerr to compare; mean = 0, std = 1\n",
    "n_test = 10000\n",
    "gaussian_dist = np.random.normal(0,1,n_test)\n",
    "test_chi = np.sum(gaussian_dist**2)\n",
    "print(r\"Chisq of ideal distribution ({:d} data points, mean=0, std=1): {:.3f}\".format(n_test,test_chi))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls = 16\n",
    "ts=14\n",
    "\n",
    "f = plt.figure(figsize=(20,7))\n",
    "plt.subplots_adjust(wspace=0.2)\n",
    "# Plot Distribution of Weighted Residuals\n",
    "ax1 = f.add_subplot(121)\n",
    "ax1.set_title(\"Distribution of Weighted Residuals\\n{:d} data points, $\\mu$ = {:.2f}, $\\sigma$ = {:.2f}\".format(num,np.mean((y-ypolyfit)/yerr),np.std((y-ypolyfit)/yerr)),fontsize=ls*1.1)\n",
    "ax1.hist((y-ypolyfit)/yerr,histtype=\"step\",bins=7)\n",
    "ax1.set_xlabel(r\"$\\frac{y(data) - y(polyfit)}{yerr}$\",fontsize=ls)\n",
    "ax1.set_ylabel(\"# data points\",fontsize=ls)\n",
    "ax1.axvline(np.mean(y-ypolyfit),c=\"k\",alpha=0.5,ls=\"dashed\")\n",
    "ax1.tick_params(axis=\"both\",labelsize=ts)\n",
    "# Plot Ideal Distribution of Residuals\n",
    "ax1 = f.add_subplot(122)\n",
    "ax1.set_title(\"Ideal Distribution (Gaussian)\\n10000 data points, $\\mu$ = 0, $\\sigma$ = 1\",fontsize=ls*1.1)\n",
    "ax1.hist(gaussian_dist,histtype=\"step\")#,bins=20)\n",
    "ax1.set_xlabel(\"Ideal Residuals\",fontsize=ls)\n",
    "ax1.axvline(0,c=\"k\",alpha=0.5,ls=\"dashed\")\n",
    "ax1.tick_params(axis=\"both\",labelsize=ts)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The quality of your chisq fit is a function of both the number of fitted parameters and the number of data points. The more data points the better the fit. The more parameters the easier it is to fit data. We encapsulate both with the concept of degrees of freedom $f = n - p$ where $n$ is the number of points and $p$ is the number of fitted parameters. We can then compare fits of different numbers of data points and parameters by looking at a statistic called the **reduced chi squared**, $\\chi_{\\nu}^2 = \\frac{\\chi^2}{f}$. A reduced chi-squared near 1 is a good fit. \n",
    "\n",
    "To determine how good it is we can determine the p-value (probability) that a higher reduced chi-squared could be due to random noise. A p-value between 0.1 and 0.9 is good. If the p_value is greater than 0.98 then your error bars are too big and if your p_value is less than 0.02 then your error bars are too small."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dof = len(y) - len(param)\n",
    "redchi = chi/dof\n",
    "print(\"Red Chisq: {:.3f} Dof: {}\".format(redchi,dof))\n",
    "p_value = 1 - stats.chi2.cdf(chi, df=dof)\n",
    "print(\"P-Value: {:.3f}\".format(p_value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In real life, our errors are often under or over estimated. For example let's say that our error bars where underestimated by a factor of 1.5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "byerr = yerr/1.5\n",
    "(bparam,bcov) = np.polyfit(x,y,1,w=1/byerr,cov=True)\n",
    "bfitp = np.poly1d(bparam)\n",
    "byfit = bfitp(x)\n",
    "bchi = np.sum(((y - byfit)/byerr)**2)\n",
    "bdof = len(y) - len(param)\n",
    "bredchi = bchi/bdof\n",
    "bp_value = 1 - stats.chi2.cdf(bchi, df=bdof)\n",
    "print(\"m= {:.3f} +/- {:.3f}\\nb= {:.3f} +/- {:.3f}\".format(bparam[0],np.sqrt(bcov[0,0]),bparam[1],np.sqrt(bcov[1,1])))\n",
    "print(\"Chisq: {:.3f}\".format(bchi))\n",
    "print(\"Red Chisq: {:.3f} Dof: {}\".format(bredchi,bdof))\n",
    "print(\"P-Value: {:.3f}\".format(bp_value)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that our fit values and errors stay the same, but our chisq is significantly different. *Note: The values and errors only stay the same because our error bars are off by a constant multiple.* Our pvalue is less than 0.02 so our error bars are too small, as we already knew."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring Chi Square Space\n",
    "We can calculate chi square values at many points and get a sense of what the chi square space looks like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Do This for our line\n",
    "mstep = 100\n",
    "bstep = 100\n",
    "b_step = np.linspace(-5,5,bstep)\n",
    "m_step = np.linspace(-5,5,mstep)\n",
    "b_arr = np.zeros(mstep*bstep)\n",
    "m_arr = np.zeros(mstep*bstep)\n",
    "chi_arr = np.zeros(mstep*bstep)\n",
    "param = np.zeros(2)\n",
    "perr = np.zeros(2)\n",
    "\n",
    "k = 0\n",
    "for i in range(len(m_step)):\n",
    "    for j in range(len(b_step)):\n",
    "        m_arr[k] = m_step[i]\n",
    "        b_arr[k] = b_step[j]\n",
    "        testy = m_arr[k]*x + b_arr[k]\n",
    "        chi_arr[k] = np.sum(((y - testy)/yerr)**2)\n",
    "        k=k+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(m_arr,b_arr,c=np.log10(chi_arr),cmap=plt.cm.plasma,marker='s',edgecolor='none')\n",
    "plt.xlabel(\"m\")\n",
    "plt.ylabel(\"b\")\n",
    "plt.colorbar(label=\"Log10(chisq)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The chi square space is really well behaved, not surprisingly. The closer we are to the right answer in either m or b, the better our chi square."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non-linear Functions\n",
    "The concepts that we discussed for linear functions also applies to non-linear functions. However the algorithms we use to find these values are much more prone to finding the wrong answer. One class of fitting algorithms are called non-linear least squares fitters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's Make Some Data\n",
    "Let's create sine curve: $Amp\\sin(\\frac{2 \\pi x}{period})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create Some data\n",
    "def mysine(x,amp,period):\n",
    "    return (amp * np.sin(2*np.pi*x/period))\n",
    "\n",
    "np.random.seed(300) #Seed the random number generator, so the result is always the same\n",
    "num = 50\n",
    "x=np.linspace(0,3,num) #Create a evenly spaced set of data points\n",
    "amp = 2.1\n",
    "period = 0.5\n",
    "\n",
    "theory_y = mysine(x,amp,period)\n",
    "\n",
    "yerr = 0.5*np.random.rand(num) + 0.5 #I want a series of sigmas between 0.5 and 1\n",
    "y = yerr * np.random.standard_normal(num) + theory_y #Move my y values by the error\n",
    "\n",
    "\n",
    "plt.errorbar(x, y, yerr=yerr,fmt='o')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We can do this the easy way or the hard way...Looks like it is going to be the hard way.\n",
    "For non-linear equations the only way to be 100% sure that you've covered chi squared space is to do a grid search. How the grid search works is that you calculate the chi squared statistic at all the values to some resolution for each of your parameters. The parameter set that has the lowest chi squared is your value. Unfortunately, for most problems a normal grid search is too computationally expensive to do very often."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "astep = 100\n",
    "pstep = 100\n",
    "amp_step = np.linspace(0,4,astep)\n",
    "period_step = np.linspace(.02,2,pstep)\n",
    "amp_arr = np.zeros(astep*pstep)\n",
    "period_arr = np.zeros(astep*pstep)\n",
    "chi_arr = np.zeros(astep*pstep)\n",
    "param = np.zeros(2)\n",
    "perr = np.zeros(2)\n",
    "\n",
    "k = 0\n",
    "for i in range(len(amp_step)):\n",
    "    for j in range(len(period_step)):\n",
    "        amp_arr[k] = amp_step[i]\n",
    "        period_arr[k] = period_step[j]\n",
    "        testy = mysine(x,amp_arr[k],period_arr[k])\n",
    "        chi_arr[k] = np.sum(((y - testy)/yerr)**2)\n",
    "        k=k+1\n",
    "#Find minimum chi square\n",
    "minidx = np.argmin(chi_arr)\n",
    "\n",
    "param[0] = amp_arr[minidx]\n",
    "param[1] = period_arr[minidx]\n",
    "chi = chi_arr[minidx]\n",
    "yfit = mysine(x,param[0],param[1])\n",
    "perr[0] = amp_step[1] - amp_step[0] #Get a step size\n",
    "perr[1] = period_step[1] - period_step[0] #Get a step size "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Note these are not Gaussian errors, only step sizes\n",
    "print(\"amp= {:.3f} +/- {:.3f}\\nperiod= {:.3f} +/- {:.3f}\".format(param[0],perr[0],param[1],perr[1]))\n",
    "chi = np.sum(((y - yfit)/yerr)**2)\n",
    "print(\"Chisq: {:.3f}\".format(chi))\n",
    "dof = len(y) - len(param)\n",
    "redchi = chi/dof\n",
    "print(\"Red Chisq: {:.3f} Dof: {}\".format(redchi,dof))\n",
    "p_value = 1 - stats.chi2.cdf(chi, df=dof)\n",
    "print(\"P-Value: {:.3f}\".format(p_value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.errorbar(x, y, yerr=yerr,fmt='o')\n",
    "plt.plot(x, yfit,'r--',lw=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's Look at Chi Square Space "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(amp_arr,period_arr,c=np.log10(chi_arr),cmap=plt.cm.plasma,marker='s',edgecolor='none')\n",
    "plt.xlabel(\"Amplitude\")\n",
    "plt.ylabel(\"Period\")\n",
    "plt.clim(1,3)\n",
    "plt.colorbar(label=\"Log10(chisq)\")\n",
    "plt.axvline(2.1,color='w',linestyle='--')\n",
    "plt.axhline(0.5,color='w',linestyle='--')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's Zoom in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(amp_arr,period_arr,c=np.log10(chi_arr),cmap=plt.cm.plasma,marker='s',edgecolor='none')\n",
    "plt.xlabel(\"Amplitude\")\n",
    "plt.ylabel(\"Period\")\n",
    "plt.clim(1,3)\n",
    "plt.xlim(1.5,2.5)\n",
    "plt.ylim(0.3,0.7)\n",
    "plt.colorbar(label=\"Log10(chisq)\")\n",
    "circle = plt.Circle((2.1,0.5), 0.05, color='b', fill=False)\n",
    "plt.gcf().gca().add_artist(circle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a very different chi square space than the line fit above. If you start at the wrong period, you will not find your way to the right one. Also it is also clear that the amplitude has trouble finding the right answer when the period is wrong."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Curve_fit\n",
    "We can use curve_fit to do a non-linear least squares fit by using our previously defined `mysine()` function. The `p0` keyword is an array of starting values for x and y. The `sigma` keyword takes the y errorbars."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(param,cov) = curve_fit(mysine,x,y,p0=[5,.51],sigma=yerr)\n",
    "yfit = mysine(x,param[0],param[1])\n",
    "perr = np.sqrt(np.diag(cov))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cov)\n",
    "cor =cov2cor(cov)\n",
    "print(cor)\n",
    "print(\"amp= {:.3f} +/- {:.3f}\\nperiod= {:.3f} +/- {:.3f}\".format(param[0],perr[0],param[1],perr[1]))\n",
    "chi = np.sum(((y - yfit)/yerr)**2)\n",
    "print(\"Chisq: {:.3f}\".format(chi))\n",
    "dof = len(y) - len(param)\n",
    "redchi = chi/dof\n",
    "print(\"Red Chisq: {:.3f} Dof: {}\".format(redchi,dof))\n",
    "p_value = 1 - stats.chi2.cdf(chi, df=dof)\n",
    "print(\"P-Value: {:.3f}\".format(p_value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.errorbar(x, y, yerr=yerr,fmt='o')\n",
    "plt.plot(x, yfit,'r--',lw=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try starting far away from the correct value in Amplitude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(param,cov) = curve_fit(mysine,x,y,p0=[100,.51],sigma=yerr)\n",
    "yfit = mysine(x,param[0],param[1])\n",
    "perr = np.sqrt(np.diag(cov))\n",
    "print(cov)\n",
    "cor =cov2cor(cov)\n",
    "print(cor)\n",
    "print(\"amp= {:.3f} +/- {:.3f}\\nperiod= {:.3f} +/- {:.3f}\".format(param[0],perr[0],param[1],perr[1]))\n",
    "chi = np.sum(((y - yfit)/yerr)**2)\n",
    "print(\"Chisq: {:.3f}\".format(chi))\n",
    "dof = len(y) - len(param)\n",
    "redchi = chi/dof\n",
    "print(\"Red Chisq: {:.3f} Dof: {}\".format(redchi,dof))\n",
    "p_value = 1 - stats.chi2.cdf(chi, df=dof)\n",
    "print(\"P-Value: {:.3f}\".format(p_value))\n",
    "plt.errorbar(x, y, yerr=yerr,fmt='o')\n",
    "plt.plot(x, yfit,'r--',lw=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We found our way to the correct values!\n",
    "\n",
    "Now let's try starting far away from the correct value of Period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(param,cov) = curve_fit(mysine,x,y,p0=[5, 2],sigma=yerr)\n",
    "yfit = mysine(x,param[0],param[1])\n",
    "perr = np.sqrt(np.diag(cov))\n",
    "print(cov)\n",
    "cor =cov2cor(cov)\n",
    "print(cor)\n",
    "print(\"amp= {:.3f} +/- {:.3f}\\nperiod= {:.3f} +/- {:.3f}\".format(param[0],perr[0],param[1],perr[1]))\n",
    "chi = np.sum(((y - yfit)/yerr)**2)\n",
    "print(\"Chisq: {:.3f}\".format(chi))\n",
    "dof = len(y) - len(param)\n",
    "redchi = chi/dof\n",
    "print(\"Red Chisq: {:.3f} Dof: {}\".format(redchi,dof))\n",
    "p_value = 1 - stats.chi2.cdf(chi, df=dof)\n",
    "print(\"P-Value: {:.3f}\".format(p_value))\n",
    "plt.errorbar(x, y, yerr=yerr,fmt='o')\n",
    "plt.plot(x, yfit,'r--',lw=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yikes! This is not the right result, our amplitude is negative, and and our period is very wrong. We could have expected this result given how sensitive chi-squared space was to period. Be careful of this sort of issue when fitting your own data. It is useful to plot your fit on your data to be sure you are at least roughly okay."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab 8: Now it is your turn\n",
    "Please answer the following questions, then print them off and turn them in. You don't need to print the whole notebook. Only print the pages starting from here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Name: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q1: Fit the following polynomial $ax^2+bx+c$ to the `x` and `y` data below. Provide the fitted values and errors on a, b,and c and plot your fit and data on the same plot.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create Some data\n",
    "np.random.seed(200) #Seed the random number generator, so the result is always the same\n",
    "num = 20\n",
    "x=np.linspace(-5,5,num) #Create a evenly spaced set of data points\n",
    "a = 1.3\n",
    "b = -3.7\n",
    "c = 2.2\n",
    "theory_y = a*x**2 + b*x + c #Let's make a line\n",
    "\n",
    "yerr = np.random.rand(num) + 1 #I want a series of sigmas between 1 and 2\n",
    "y = yerr * np.random.standard_normal(num) + theory_y #Move my y values by the error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q2: Compute the $\\chi^2$, $\\chi_{\\nu}^2$, and p-value for your fit in Q1.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q3:You are studying the growth rate of an alien bacterial colony. Create and fit Gaussian data for $y = Ae^{t/\\lambda}$ where $A = 10$, $\\lambda = 100s$, and $\\sigma=40$ with 20 evenly spaced data points in time. t goes from 0 to 400s. Give your fitted parameters, $\\lambda$ and A, plus errorbars. Plot your fit and data on the same plot.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create Some Data\n",
    "np.random.seed(300) #Seed the random number generator, so the result is always the same"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
