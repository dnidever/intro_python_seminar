{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bdde34a1-0d5e-4162-bd6c-c6fc305c0704",
   "metadata": {},
   "source": [
    "# Introduction to Machine Learning\n",
    "\n",
    "This Notebook is designed to give students a general feel for how machine learning works. This will be done through a couple of steps:\n",
    "1. Defining Machine Learing\n",
    "2. Outlining the general types of Machine Learning algorithms/approaches\n",
    "3. Implementing a single-layer and a multi-layer Nueral Network via `scikit-learn`\n",
    "4. Implementing a MLP to classify numbers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed32589a-191c-42a6-ade1-78414ab97db0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Defining Machine Learning\n",
    "\"**Definition:** A computer program is said to **learn** from experience $E$ with respect to some class of tasks $T$ and performance measure $P$, if its performance at tasks in $T$, as measured by $P$, improves with experience $E$\"<br>\n",
    "> *Machine Learning* by Tom M. Michell\n",
    "\n",
    "\"Machine learning can be broadly defined as computational methods using experience to improve performance or to make accurate predictions.\"\n",
    "> *Foundations of Machine Learning* by Mehryar Mohri, Afshin Rostamizadeh, and Ameet Talwalkar\n",
    "\n",
    "Many methods that we don't recognize are Machine Learning (ML) algorithms, are. For example, if you've ever used a Markov Chain Monte Carlo (MCMC) algorithm to solve an integral or a Particle Swarm Optimization (PSO) to minimize an equation you've used ML. Most algorithms that do clustering and predict probability density functions are machine learning algorithms. ML is often described as 'data driven algorithms.' Along this concept, anything that scours databases for information or patterns, often called data mining, are ML algorithms to some extent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c85d1c0-653e-426a-9a0c-b6efe120ac11",
   "metadata": {},
   "source": [
    "# Types of Machine Learning Algorithms\n",
    "As you can already tell, ML covers a vast section of algorithms. Let's take a look at a couple different types of ways of organizing them.\n",
    "## Standard Learning Tasks\n",
    "We can organize algorithms by what they are trying to do:\n",
    "- *Classification*: Trying to discover a mapping of some function $f: \\vec{v} \\to C$, where $\\vec{v}$ is a vector of input values and $C$ is the collection of classes we are trying classify into. There is a difficult discussion that be had around the concepts of black-box models vs. white-box models, understandability of models, interpretability, verifyability, etc. \n",
    "- *Regression*: Trying to predict a real valued (there are extension that include imaginary values and higher order tensors) from the input characterisitics; $f: \\vec{v} \\to \\mathbb{R}$.\n",
    "- *Ranking*: Trying to learn how to order items according to some criterion.\n",
    "- *Clustering*: Trying to find logical groups in the data.\n",
    "- *Dimensionality reduction* or *Manifold Learning*: Trying to transform a higher-dimensional input vector into a lower-dimensional space or one with preferable characteristics.\n",
    "\n",
    "## Learning Scenarios\n",
    "Often we classify algorithms about the type of information that is fed to it:\n",
    "- *Supervised Learning$^\\dagger$*: The data fed to the algorithm is fully labeled with the respective classes or real valued output.\n",
    "- *Unsupervised Learning$^\\dagger$*: The data fed to the algorithm is completely unlabeled. This results usually running \"algorithms\" to find patterns instead of running programs to generate \"models.\"\n",
    "- *Semi-supervised Learning*: Some of the data is labeled, but not all of it. This usually has iterative steps of training a Supervised Learning model off the labeled data, guessing labels for the unlabeled points, then training a model from the newly labeled points. (Thanks to work initially done by Dempster, this is shown to converge under specific criteria.)\n",
    "- *Reinforcement$^\\dagger$*: To collect data, the agent interacts with the environment.\n",
    "- *Transductive Inference*: A special case of Semi-supervised Learning where the objective is only to label the given points, without regards to generalizability of the model.\n",
    "- *On-line Learning*: Data is collected in iterations of tarining and testing phases. This gives models that improves as they are used.\n",
    "- *Active Learning*: Data is collected in collaboration with an oracle/teacher to answer requests for new data points for the algorithm. \n",
    "\n",
    "$^\\dagger$ These are often the only three that are referred to, and the others can be described with respect to them.\n",
    "\n",
    "## Purpose\n",
    "When you want to apply a ML program, these concepts can help educate you on what ML algorithm to choose. Think about what the objective you want to solve in your program? What data do you have to provide? How well does it need to generalize? How understandable does the model need to be? Are you limited in the amount of time or computational resources?\n",
    "\n",
    "If you are interested in applying ML, I would urge you to take a glance through Google's free training on ML. Specifically, they outline the thoughts that go on behind properly framing the question: [Machine Learning: Problem Framing (Google)](https://developers.google.com/machine-learning/problem-framing)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09cfc214-9da2-447c-a39c-1901135974f2",
   "metadata": {},
   "source": [
    "Okay, that's enough of being pedantic and defining what Machine Learning is. How about we look at one of the hottest ML algorithm, the Neural Network?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66e69cb8-6d32-4ea4-be17-0eba86cfe671",
   "metadata": {},
   "source": [
    "# Building ML Programs\n",
    "\n",
    "There is no reason to reinvent the wheel, so let's use Scikit Learn\n",
    "\n",
    "    pip install sklearn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "74cb132d-0a0d-4bd3-9ed4-e87806f96949",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np #as always\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.neural_network import MLPClassifier #this brings in the Multi-Level Perceptron Classificer for us to use"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "056c52b7-2f2e-4c5f-995b-e9d718bd81e2",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "We will be using the Multi-Level Perceptron. The structure of the object works as an iterative nesting of a collection of linear functions.\n",
    "\n",
    "## Perceptron\n",
    "The original perceptron construction, what we might call a single-layer feed-forward neural-network or a linear neural-network, tries to learn the parameters for the function \n",
    "$$f(x; \\theta) = \\mathbb{I}(w^T x +b \\geq 0) = H(w^T x + b)$$\n",
    "where $w$ is some learned weight function, $H$ is the Heavyside function. In many applicatioins, this *activation function* is replaced by functions such as RELU and the Sigmoid function. In the original Perceptron was designed by Rosenblatt in 1958, and later the *Perceptrons* book by Minsky and Papert in 1969 was proven to be only able to learn linear functions. If we chain these functions together, however, we can start deriving a nonlinear problem.\n",
    "$$f(x) = f_4 \\circ f_3 \\circ f_2 \\circ f_1(x) = f_4(f_3(f_2(f_1(x))))$$\n",
    "\n",
    "This is an identical formalization using the illustration of fig. 1.\n",
    "![Multi-Level Perceptron Illustration borrowed from https://www.ibm.com/cloud/learn/neural-networks ](data\\ML_resources\\images\\neural_networks1.png)\n",
    "*fig 1.* Multi-Level Perceptron Illustration borrowed from https://www.ibm.com/cloud/learn/neural-networks \n",
    "\n",
    "Each of the layers represents a function in the construction above, and the edge weights that are learned can be thought of as the elements that make up the weights $w$. Not pictured here, there is some defined activation function at all these weights and input vectors will be fed into. \n",
    "\n",
    "The construction of the MLP was as far as researchers got on this problem until around the 1980's. At this point it was unclear how to train these multi-layer networks. Today, the common approach used is the **Backpropagation** algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a74f8abf-3c1a-42e9-a870-9b10c23871e0",
   "metadata": {},
   "source": [
    "## Pulling in data\n",
    "Before we can start doing anythin fancy, we have to collect the data to use. For my own learning I like to use the [UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/index.php). I have selected two datasets for us to train our MLP's from. The first is a collection of Breast Cancer data, called the [Breat Cancer Wisconsin (Diagnostic) Data Set](https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+%28Diagnostic%29). The second is the ... . Perceptrons require real-valued data points, and SciKit-Learn's MLP does not support missing data points (it is still an open research topic about the best way to handle missing data in Artificial Neural-Networks). \n",
    "\n",
    "I have taken the liberty to import the data and format it correctly, making sure all values are real valued and removing the few data-points with missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4b128510-18d4-4c9c-b41d-e887fab29502",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def import_data(name=\"./data/ML_resources/breast_cancer_wisconsin/data.csv\", training_percent=.80, ):\n",
    "    data = np.genfromtxt(name, delimiter=\",\")    #Let Numpy do the heavy lifting and process the data\n",
    "    nan_locs = np.where(np.isnan(data) == True)    #Locate which rows have missing data\n",
    "    data = np.delete(data, nan_locs, 0)    #Remove rows with missing data\n",
    "\n",
    "    rng = np.random.default_rng()    #This is numpy's prefered Generator to do random methods\n",
    "    \n",
    "    rng.shuffle(data)    #Shuffle my data uniformly, so we can get a random sample\n",
    "    training = data[0:int(data.shape[0] * training_percent), :]    #Generate a training set according to given proportion\n",
    "    testing = data[int(data.shape[0] * training_percent)::, :]    #Generate a testing set according to what is not trained on\n",
    "    return training, testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "246c2545-6d9f-4632-aebd-b218de139f81",
   "metadata": {},
   "source": [
    "You should notice that this function already divides out a training, and testing set. This is common practice, as we would like to know how good our model is performing. A common number to choose is 80%, but you will see later that we choose to run with 90% since our datasets are small and would like as much training as possible.\n",
    "\n",
    "Next, let us initiate our MLPClassifier object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d3e12571-1f92-409a-ac05-f69d1fbdc22e",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=tuple([5]*3), random_state=1, max_iter=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc1f5104-4403-432e-ba42-608865ef8680",
   "metadata": {},
   "source": [
    "- We have given the chosen a family of quasi-Newton methods for our differentiation method (`solver='lbfgs'`), according to the documentation this is good for small datasets.\n",
    "- The code will stop running after 1000 iterations\n",
    "- We have provided an $\\alpha$, or learning rate, or $1 \\times 10^{-5} = 0.00001$. This is a tunable hyperparameter.\n",
    "- The construction has 3 hidden layers, or layers beyond the input and output layer, and each with 5 nodes. \n",
    "- I have provided the starting seed, random_state, to show how the starting position can effect the final model's performance.\n",
    "\n",
    "At this point, we are ready to make the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d52d4c78-cea4-4e4c-84db-af379ce754f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLPClassifier(alpha=1e-05, hidden_layer_sizes=(5, 5, 5), max_iter=1000,\n",
       "              random_state=1, solver='lbfgs')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train, test = import_data(training_percent=0.9)\n",
    "features = train[:, 0:-1]\n",
    "classes = train[:, -1]\n",
    "\n",
    "clf.fit(features, classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6b2163f-9000-4c1a-9983-e6cab84cc26e",
   "metadata": {},
   "source": [
    "To test the the performance, we can run a collection of testing points through the model via the `score` method. This will return the percent that were classified correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e6a6240a-b98a-4061-960b-b9158c31848e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9130434782608695"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_test = test[:, 0:-1]\n",
    "classes_test = test[:, -1]\n",
    "\n",
    "clf.score(features_test, classes_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92de994e-c196-454d-b2db-e21802825c58",
   "metadata": {},
   "source": [
    "That's some horrible performance. The performance is barely better than random (50%), or what we would call a weak learner. Let's try running a couple different hyperparameters to see if it was a specific contruction that caused our poor performance, or if it was the model chosen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e74c304d-7806-43b7-bc3b-96fca0553eb3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def MLP_test_bcw(trials, alpha=1e-5, layers=5, random_state=None, iterations=1000):\n",
    "    results = []\n",
    "    for i in range(trials):\n",
    "        clf = MLPClassifier(solver='lbfgs', alpha=alpha, hidden_layer_sizes=(layers, 2), random_state=random_state, max_iter=iterations)\n",
    "        train, test = import_data(training_percent=0.9)\n",
    "        clf.fit(train[:, 0:-1], train[:, -1])\n",
    "        results.append(clf.score(test[:, 0:-1], test[:, -1]))\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fede07d2-cf5b-4fae-a171-9969fb6c5192",
   "metadata": {},
   "outputs": [],
   "source": [
    "trial_1 = MLP_test_bcw(15, layers=5, random_state=1)\n",
    "trial_2 = MLP_test_bcw(15, layers=5, random_state=2)\n",
    "trial_3 = MLP_test_bcw(15, layers=5, random_state=3)\n",
    "plt.boxplot([trial_1, trial_2, trial_3], vert=False, labels=['1', '2', '3'])\n",
    "plt.title(\"Testing NN Seeds\")\n",
    "plt.xlabel('Accuracy')\n",
    "plt.ylabel('random_state (seed) value')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66d480eb-f6cd-4e09-9c32-14d66d901044",
   "metadata": {},
   "outputs": [],
   "source": [
    "trial_1 = MLP_test_bcw(15, layers=1, random_state=3)\n",
    "trial_2 = MLP_test_bcw(15, layers=3, random_state=3)\n",
    "trial_3 = MLP_test_bcw(15, layers=5, random_state=3)\n",
    "trial_4 = MLP_test_bcw(15, layers=7, random_state=3)\n",
    "\n",
    "plt.boxplot([trial_1, trial_2, trial_3, trial_4], vert=False, labels=['1', '3', '5', '7'])\n",
    "plt.title(\"Testing NN Depth\")\n",
    "plt.xlabel('Accuracy')\n",
    "plt.ylabel('Number of layers')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "94864179-1203-4049-bfaa-5fbfb755fb11",
   "metadata": {},
   "source": [
    "# On slower computers, the below tests may take too long. If that is so, change that block to \n",
    "# `Raw` format and change this to `Code`.\n",
    "\n",
    "trial_3 = MLP_test_bcw(15, layers=5, alpha=1e-7, random_state=3)\n",
    "trial_4 = MLP_test_bcw(15, layers=5, alpha=1e-9, random_state=3)\n",
    "trial_5 = MLP_test_bcw(15, layers=5, alpha=1e-11, random_state=3)\n",
    "\n",
    "plt.boxplot([trial_3, trial_4, trial_5], vert=False, labels=['1e-7', '1e-9', '1e-11'])\n",
    "plt.title(\"Testing NN Learing Rate\")\n",
    "plt.xlabel('Accuracy')\n",
    "plt.ylabel('Learning Rate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fc22a5e-6cda-4c5e-9ded-328caaf008e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "trial_1 = MLP_test_bcw(15, layers=5, alpha=1e-3, random_state=3)\n",
    "trial_2 = MLP_test_bcw(15, layers=5, alpha=1e-5, random_state=3)\n",
    "trial_3 = MLP_test_bcw(15, layers=5, alpha=1e-7, random_state=3)\n",
    "trial_4 = MLP_test_bcw(15, layers=5, alpha=1e-9, random_state=3)\n",
    "trial_5 = MLP_test_bcw(15, layers=5, alpha=1e-11, random_state=3)\n",
    "trial_6 = MLP_test_bcw(15, layers=5, alpha=1e-1, random_state=3)\n",
    "trial_7 = MLP_test_bcw(15, layers=5, alpha=1, random_state=3)\n",
    "trial_8 = MLP_test_bcw(15, layers=5, alpha=1e-13, random_state=3)\n",
    "\n",
    "plt.boxplot([trial_7, trial_6, trial_1, trial_2, trial_3, trial_4, trial_5, trial_8], vert=False, \n",
    "            labels=['1', '1e-1', '1e-3', '1e-5', '1e-7', '1e-9', '1e-11','1e-13'])\n",
    "plt.title(\"Testing NN Learing Rate\")\n",
    "plt.xlabel('Accuracy')\n",
    "plt.ylabel('Learning Rate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26045246-473a-40ed-a9b6-8f048cea130f",
   "metadata": {},
   "source": [
    "# Now it is your turn\n",
    "\n",
    "We are processing a document so that the numbers are read automatically. Our original file is this passport (you might recognize it from the game *Papers, Please*).\n",
    "\n",
    "![Image to take numbers from. Screen catured from Papers Please](data\\ML_resources\\images\\passport.png)\n",
    "\n",
    "I have gone ahead and grabbed all the numbers to be processed. They look something like\n",
    "\n",
    "![The number 3](data\\ML_resources\\objective\\3_1.png)\n",
    "![The number 3](data\\ML_resources\\objective\\3_2.png)\n",
    "![The number 4](data\\ML_resources\\objective\\4_0.png)\n",
    "\n",
    "We will be training our data with a handwritten numbers set from [UCI database](https://archive.ics.uci.edu/ml/datasets/Semeion+Handwritten+Digit). Download the data `semeion.data` file from the `data folder`. Move it into `intro_python_seminar\\notebooks\\data\\semion\\`. I prefer to name all my data files `data.csv` and deliminate which dataset I am in by the name of the folder for better organization and the ability to automate trials."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7c1acd8-db96-4b7a-8142-c6e3b922015f",
   "metadata": {},
   "source": [
    "### Import the data\n",
    "It is time to import the data. These 16x16 images are 276 element long arrays: (256 image elements, 10 elements one-hot encoded vector). One-hot encoding a vector is when you transform a categorical variable into a Boolean vector, where the appropriate element is `1` if it is that class, and `0` for all other classes. For example, if we one-hot encoded the set $\\{1,2,3\\}$, we get \n",
    "$$1 \\rightarrow [1,0,0]$$\n",
    "$$2 \\rightarrow [0,1,0]$$\n",
    "$$3 \\rightarrow [0,0,1]$$\n",
    "So, when we pass our train set, we will need to omit the last 10 elements, something like:\n",
    ">```train[:, :-10], train[:, -10::]```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83df1e49-8e8a-4b56-b1cb-e3ca4166d1b2",
   "metadata": {},
   "source": [
    "**Q1: Download the dataset from the UCI and import the data, using the same method we used before; I would adivse that you save the data into a `train` and `test` matrix like before. (If you are having trouble getting the data, I have left a copy of the csv named `backup_data.csv` in the appropriate folder.)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9e7bd84-71c6-4773-b2c0-128acf933f8d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d303dedb-7dc0-4eea-9318-302e4345de36",
   "metadata": {},
   "source": [
    "### Building the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c097d053-5848-4b62-a223-461882fa86d8",
   "metadata": {},
   "source": [
    "**Q2: Build a new MLP object and place it into `number_model`. You should use the `solver='lbfgs'`, 7 layers with 100 nodes. We need to use the `lbfgs` solver because otherwise our model will not start to converge because of our dataset size. If you are having trouble with runtime, try reducing to 2 layers with 100 nodes.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59fc47d3-08c5-41ab-a345-d4d68f2a8a25",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d7e8c627-7cad-4664-803b-1a6afeb9d93c",
   "metadata": {},
   "source": [
    "**Q3: Train your model with the training set created in Q1. Use the test set defined in Q1 to get a score for your model. If you did everything correctly you should have a score around $.80-.90$**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3b6be4b-747f-441a-996c-c4214c89f384",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "451a8507-ffbf-4ae8-a341-11d46da4e0c2",
   "metadata": {},
   "source": [
    "### Classifying Numbers\n",
    "We are now ready to try to classify our numbers. The data needs to be pre-processed so that it is in the same format the data we trained our model on was. The function below reads in a given image and transforms it to an inverse, binary, greyscale vector image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7702e2f4-62df-4365-96f2-8d8d81a130ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.image as img    #We are using matplotlib.image to import our images\n",
    "\n",
    "def process_num(name, objective_folder = \".\\\\data\\\\ML_resources\\\\objective\"):\n",
    "    image = img.imread(objective_folder + \"\\\\\" + name)   #Read in the image\n",
    "    image = np.sum(image, axis=2)**-1    #Invert greyscale - this is how our training data was\n",
    "    image = image / image.max()    #Scale our files\n",
    "    image = image.flatten().reshape([1, -1])    #Reshape for the next step\n",
    "    image = np.array([0 if x < np.median(num) else 1 for x in num[0]]).reshape(1,-1)    #Transform to boolean with respect to median\n",
    "    return image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be8ca3ac-5542-40f4-b9b3-a0dfeba2b0ac",
   "metadata": {},
   "source": [
    "**Q4: Read in `1_1.png` and using the `.predict` method from our model to classify our image. Assuming our model is behaving well, we should get something like:**\n",
    ">`array([[0, 1, 0, 0, 0, 0, 0, 0, 0, 0]])`\n",
    "\n",
    "**If you did not get a vector like that out, try rerunning Q3, it might be a bad seed. However, if it does not after trying two different seed, chances are there is a bug in your code or you need a larger network (perhaps you need a larger training proportion?).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7c15f26-5457-4302-88b7-2f028c744a68",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c8fd2b8b-56fe-499f-a8d7-454e286ea122",
   "metadata": {},
   "source": [
    "**Q5: If we run `predict` on other number (try `2_0.png`) we might get**\n",
    ">`array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])`\n",
    "\n",
    "**because there is no adequately large \"hit.\" The solution is getting out the final activation of nodes, or the probability distribution of what the solution is. Get the `predict_proba` or `predict_log_proba` for `1_1.png`. Normalize this result using `np.linalg.norm`. This might look like**\n",
    "\n",
    "```\n",
    "pred = model.predict_proba(1_1_data)\n",
    "pred = pred / np.linalg.norm(pred)\n",
    "```\n",
    "\n",
    "**Run your code on one or two of the other images to make sure it gives you a resonable result.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f1317f5-3a68-405c-a9ca-ebc1e1b49bb4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "91bf62e6-599a-4973-a0c5-5a3165ed6cf0",
   "metadata": {},
   "source": [
    "**Q6: Write a short function that takes in the name of an image, then returns a vector of normalized probability of what number it is. Run this function on all the images in the `data\\ML_resources\\objective` folder.**\n",
    "\n",
    "*Hint: if your results seem off, take a look down at Q6 before you start stressing.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a79c827b-5c09-44d9-93eb-3806d7ea0908",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "10d0f858-2e93-4e61-ac09-42733a57b18d",
   "metadata": {},
   "source": [
    "**Q7: From Q5, you should be seeing some incorrect answers. For example, I got that `1_0.png` is most likely 0 or 8, not 1. For the numbers that are far off, refer to the table below and come up with some reasons that they might have been classified wrong.**\n",
    "\n",
    "The method that we have imployed here is a very naive one. In order to start getting accurate results, we would want to do use data that more closely reflects our problem (such as full greyscale, larger dataset, training on computer rendered images), and/or use a more sophisticated model. If you are interested in this, you might be interested in reading up on Convolutional Neural Networks (My own research is likely going to be in CNNs on Hyperspectral image data)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "939490ba-a196-4995-8a76-07d03a13681a",
   "metadata": {},
   "source": [
    "|File|Image|\n",
    "|---:|:--:|\n",
    "|`1_0.png`|![The number 1](data\\ML_resources\\objective\\1_0.png)|\n",
    "|`1_1.png`|![The number 1](data\\ML_resources\\objective\\1_1.png)|\n",
    "|`1_2.png`|![The number 1](data\\ML_resources\\objective\\1_2.png)|\n",
    "|`2_0.png`|![The number 2](data\\ML_resources\\objective\\2_0.png)|\n",
    "|`2_1.png`|![The number 2](data\\ML_resources\\objective\\2_1.png)|\n",
    "|`3_0.png`|![The number 3](data\\ML_resources\\objective\\3_0.png)|\n",
    "|`3_1.png`|![The number 3](data\\ML_resources\\objective\\3_1.png)|\n",
    "|`3_2.png`|![The number 3](data\\ML_resources\\objective\\3_2.png)|\n",
    "|`4_0.png`|![The number 4](data\\ML_resources\\objective\\4_0.png)|\n",
    "|`5_0.png`|![The number 5](data\\ML_resources\\objective\\5_0.png)|\n",
    "|`9_0.png`|![The number 9](data\\ML_resources\\objective\\9_0.png)|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01ab753d-0059-44fa-bc63-cc2c54ef66ac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
